{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sadra-barikbin/persian-sentiment-analysis-example/blob/main/sentiment-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY0mx4pxnvWT"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTuTM0pgtjjp"
   },
   "outputs": [],
   "source": [
    "!pip install clean-text[gpl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "naughty-milton"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cleantext\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a18VhnUanxhq"
   },
   "source": [
    "# Loading & Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attached-chocolate"
   },
   "outputs": [],
   "source": [
    "!gdown 'https://drive.google.com/uc?id=1HH8QFDcvkKfnj4dWmFQceb3PpNqDD8HQ&authuser=0&export=download'\n",
    "!gdown 'https://drive.google.com/uc?id=1uDOO8RP7Lr9qcRJO8z3d10qm_UggJv4I&authuser=0&export=download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "christian-number"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "eval_df = pd.read_csv('eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0Mcw7kGmoWyi",
    "outputId": "cb42e531-39c4-4bc6-caa0-a5d88f2a8009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2587</td>\n",
       "      <td>پردازنده های Core i5 و Core i3 نیز ذاتا دو هست...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22591</td>\n",
       "      <td>سلام به دوستای عزیزم \\nعزاداری هاتون قبول باشه</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141037</td>\n",
       "      <td>کلا پولتون رو دور نریزیزد</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58593</td>\n",
       "      <td>از صمیم قلب امیدوارم دایانا با کارن بمونه و پو...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5712</td>\n",
       "      <td>آنطور که اپل ادعا می کند آیپاد شافل دارای طراح...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  rate\n",
       "0        2587  پردازنده های Core i5 و Core i3 نیز ذاتا دو هست...   0.0\n",
       "1       22591     سلام به دوستای عزیزم \\nعزاداری هاتون قبول باشه   1.0\n",
       "2      141037                          کلا پولتون رو دور نریزیزد  -1.0\n",
       "3       58593  از صمیم قلب امیدوارم دایانا با کارن بمونه و پو...   1.0\n",
       "4        5712  آنطور که اپل ادعا می کند آیپاد شافل دارای طراح...   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L4tBlfcToYmg",
    "outputId": "78b5e684-f6ce-41d1-cea2-d481f44b4463"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61591</td>\n",
       "      <td>کیفیت غذا و زمان رسیدن عالی بود</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50299</td>\n",
       "      <td>در‌ حد ساندویچ یه نفره بود نه دونفره یا بمب. ک...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2777</td>\n",
       "      <td>طعم پیتزای چهار فصل مثل همشه خیلی خوب بود اما ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9126</td>\n",
       "      <td>مشخصات سخت افزاری مناسب در کنار سیستم عامل وین...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7544</td>\n",
       "      <td>مرغش سوخاری و خوشمزه بود، بسته بندی عالی، قیمت...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  rate\n",
       "0       61591                    کیفیت غذا و زمان رسیدن عالی بود  -1.0\n",
       "1       50299  در‌ حد ساندویچ یه نفره بود نه دونفره یا بمب. ک...   1.0\n",
       "2        2777  طعم پیتزای چهار فصل مثل همشه خیلی خوب بود اما ...  -1.0\n",
       "3        9126  مشخصات سخت افزاری مناسب در کنار سیستم عامل وین...   0.5\n",
       "4        7544  مرغش سوخاری و خوشمزه بود، بسته بندی عالی، قیمت...  -1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9hppW6kwoc9N"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(train_df.columns[0], axis=1)\n",
    "eval_df = eval_df.drop(eval_df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIVevpfPonAK",
    "outputId": "233e883d-8f34-4c75-c631-1acc60687d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 out of 800 train comments have rate zero.\n",
      "30 out of 200 eval comments have rate zero.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_df[train_df.rate == 0])} out of {len(train_df)} train comments have rate zero.\")\n",
    "print(f\"{len(eval_df[eval_df.rate == 0])} out of {len(eval_df)} eval comments have rate zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fN3mSVdDp656"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.rate != 0]\n",
    "eval_df  = eval_df[eval_df.rate != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7QKtjCFpo9xI"
   },
   "outputs": [],
   "source": [
    "train_df['rate'] = train_df.rate.apply(lambda r: 1 if r > 0 else 0)\n",
    "eval_df['rate'] = eval_df.rate.apply(lambda r: 1 if r > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ppN5iFXqUqC"
   },
   "source": [
    "## Balancing Dataset\n",
    "As you can see below, data is imbalanced. We use over-sampling strategy on negative class to mitigate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7UcccdyZrvkJ",
    "outputId": "00abcb47-05f6-4b28-9a1b-3781ab13ecce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train  eval\n",
       "1    502   115\n",
       "0    194    55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([train_df.rate.value_counts().rename('train'),\n",
    "           eval_df.rate.value_counts().rename('eval')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kawgs4h-q01l"
   },
   "outputs": [],
   "source": [
    "balancer = RandomOverSampler(random_state=41)\n",
    "train_df, _ = balancer.fit_resample(train_df, train_df.rate)\n",
    "eval_df, _ = balancer.fit_resample(eval_df, eval_df.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_EwWGNEtZpE"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eCIhQtnNtcqC"
   },
   "outputs": [],
   "source": [
    "params = {'to_ascii':False, 'no_urls':True,    'no_phone_numbers':True, 'no_line_breaks':True,\n",
    "          'no_emails':True, 'no_numbers':True, 'no_digits':True,        'no_currency_symbols':True}\n",
    "\n",
    "train_df['comment'] = train_df.comment.apply(lambda c: cleantext.clean(c,**params))\n",
    "eval_df['comment'] = eval_df.comment.apply(lambda c: cleantext.clean(c,**params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60271"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"vocab.txt\") as fp:\n",
    "    words = set([w.strip() for w in fp.readlines()])\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for c in train_df['comment']:\n",
    "    words.update(word_tokenize(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as fw:\n",
    "    for w in vocab:\n",
    "        fw.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60273"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcU7TIPZ2pfG"
   },
   "source": [
    "# Method 1: Linear Models\n",
    "We make use of Logistic Regression and SVM as classifiers, and for vectorizing the comments, Tfidf is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "v2_juBOy3xIW"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=600, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOPXtNPEww7e",
    "outputId": "e5ef5b70-3ddf-421a-9373-ed0b3d8aa918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('embedding',\n",
       "                 TfidfVectorizer(max_features=600, ngram_range=(1, 3))),\n",
       "                ('classifier', LinearSVC())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('embedding', vectorizer),\n",
    "                     ('classifier', LinearSVC())])\n",
    "pipeline.fit(train_df.comment, train_df.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHTNpGAl4LDP"
   },
   "source": [
    "## Hyper-parameter Tuning\n",
    "We search over different settings and find the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6_XBYuBT4R0V"
   },
   "outputs": [],
   "source": [
    "param_grid = {'embedding__ngram_range': [(1,2),(1,3),(1,4)],\n",
    "              'embedding__max_features': range(100, 3000, 100),\n",
    "              'classifier': [LinearSVC(),LogisticRegression()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rnv3weP4-gP",
    "outputId": "401d7e20-e563-4453-85da-c8ef79e650f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 174 candidates, totalling 174 fits\n"
     ]
    }
   ],
   "source": [
    "# The smelling code here is due to Scikit GridSearchCV's specific input for `cv` parameter.\n",
    "# GridSearchCV and other meta-estimators in Scikit accept whole data (train+eval) in their `fit`\n",
    "# method. So if you have a dataset separated in train and eval parts beforehand, you should\n",
    "# concatenate them. Beside that you have to give indices of train and eval parts as the `cv` parameter.\n",
    "\n",
    "train_eval = pd.concat((train_df, eval_df), ignore_index=True)\n",
    "train_eval_indices = [(train_df.index, eval_df.index + len(train_df))]\n",
    "meta_estimator = GridSearchCV(pipeline, param_grid, scoring=['accuracy', 'f1'],\n",
    "                              cv=train_eval_indices, refit='f1', verbose=1)\n",
    "_ = meta_estimator.fit(train_eval.comment, train_eval.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxJhaX4YDuhs",
    "outputId": "18a8ce1e-c9ba-4b52-842f-9483ef864047"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LinearSVC(),\n",
       " 'embedding__max_features': 600,\n",
       " 'embedding__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_estimator.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsTd-cnTD8SJ",
    "outputId": "46fb5bcf-c04b-4ef9-e58a-80c99c966eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model F1: 0.7063197026022304\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model F1: {meta_estimator.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3obiJVnpEkbe"
   },
   "source": [
    "## Determining Marker Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSqPMgiBEn9H",
    "outputId": "4a0dcc2a-1e62-403f-84fc-4676ea80548f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ارسال', 'اینکه', 'لک', 'معمولی', 'من خیلی', 'میدان', 'نيست',\n",
       "       'هم که', 'چندان', 'کاملا'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectFromModel(pipeline, threshold=-np.inf, max_features=10, prefit=True,\n",
    "                           importance_getter='named_steps.classifier.coef_')\n",
    "\n",
    "pipeline['embedding'].get_feature_names_out()[selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXg2hxaADT0m"
   },
   "source": [
    "# Method 2: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MTb3Yk6_DY69"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 200\n",
    "SEQ_LEN = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "BATCH_SIZE = 20\n",
    "CLASS_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['comment'], train_df['rate'])).shuffle(buffer_size=len(train_df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "eval_ds = tf.data.Dataset.from_tensor_slices((eval_df['comment'], eval_df['rate'])).shuffle(buffer_size=len(eval_df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Embedding, Input, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_lstm_model(lstm_dim=64, dense_dim=16):\n",
    "    vectorizer = TextVectorization(vocabulary='vocab.txt',\n",
    "                                   output_mode='int', \n",
    "                                   output_sequence_length=SEQ_LEN)\n",
    "    return Sequential([\n",
    "        Input(shape=(1,), dtype=tf.string),\n",
    "        vectorizer,\n",
    "        Embedding(VOCAB_SIZE + 4, EMBED_DIM, name='embedding'),\n",
    "        Bidirectional(LSTM(lstm_dim)),\n",
    "        Dense(dense_dim, activation='relu'),\n",
    "        Dense(CLASS_NO, 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lstm_model()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60275"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers[0].get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 64)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 64, 200)           12055000  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              135680    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2064      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,192,778\n",
      "Trainable params: 12,192,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "51/51 [==============================] - 14s 207ms/step - loss: 0.6790 - accuracy: 0.5916 - f1_m: 0.6463 - val_loss: 0.6745 - val_accuracy: 0.6000 - val_f1_m: 0.6604\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 10s 195ms/step - loss: 0.3515 - accuracy: 0.8855 - f1_m: 0.6558 - val_loss: 0.9430 - val_accuracy: 0.6130 - val_f1_m: 0.6616\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 10s 191ms/step - loss: 0.0733 - accuracy: 0.9801 - f1_m: 0.6623 - val_loss: 1.2968 - val_accuracy: 0.6130 - val_f1_m: 0.6530\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 10s 189ms/step - loss: 0.0254 - accuracy: 0.9920 - f1_m: 0.6569 - val_loss: 1.3554 - val_accuracy: 0.6261 - val_f1_m: 0.6655\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 10s 193ms/step - loss: 0.0131 - accuracy: 0.9960 - f1_m: 0.6586 - val_loss: 1.8099 - val_accuracy: 0.6304 - val_f1_m: 0.6607\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 10s 193ms/step - loss: 0.0112 - accuracy: 0.9960 - f1_m: 0.6631 - val_loss: 1.9341 - val_accuracy: 0.6348 - val_f1_m: 0.6638\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 10s 189ms/step - loss: 0.0090 - accuracy: 0.9960 - f1_m: 0.6503 - val_loss: 1.8458 - val_accuracy: 0.6261 - val_f1_m: 0.6607\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 10s 195ms/step - loss: 0.0069 - accuracy: 0.9970 - f1_m: 0.6540 - val_loss: 2.0690 - val_accuracy: 0.6348 - val_f1_m: 0.6407\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 10s 193ms/step - loss: 0.0060 - accuracy: 0.9970 - f1_m: 0.6564 - val_loss: 2.1402 - val_accuracy: 0.6348 - val_f1_m: 0.6535\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 10s 193ms/step - loss: 0.0082 - accuracy: 0.9980 - f1_m: 0.6613 - val_loss: 2.1468 - val_accuracy: 0.6304 - val_f1_m: 0.6595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45be82c5e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=eval_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6iqEzrQDYG5"
   },
   "source": [
    "# Method 3: Pre-trained Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NG5EEcYELKN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "sentiment-classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
