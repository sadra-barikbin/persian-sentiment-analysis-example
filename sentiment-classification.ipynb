{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadra-barikbin/persian-sentiment-analysis-example/blob/main/sentiment-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY0mx4pxnvWT"
      },
      "source": [
        "# Setup"
      ],
      "id": "IY0mx4pxnvWT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTuTM0pgtjjp",
        "outputId": "70e323c0-567c-4aa8-b9e1-d61885140ef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clean-text[gpl]\n",
            "  Downloading clean_text-0.5.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]) (0.2.5)\n",
            "Building wheels for collected packages: ftfy, emoji\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=f87b4c8e1ffaace88bcd0c4045a4c07526dd046f1f2348e955a578301ec888db\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=f5a3e6f10b77c476666a5d179923e5390264ddc1a0a3fb688bf81fb3c799fe4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built ftfy emoji\n",
            "Installing collected packages: ftfy, emoji, unidecode, clean-text\n",
            "Successfully installed clean-text-0.5.0 emoji-1.6.1 ftfy-6.0.3 unidecode-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text[gpl]"
      ],
      "id": "RTuTM0pgtjjp"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "naughty-milton"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import cleantext\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "id": "naughty-milton"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a18VhnUanxhq"
      },
      "source": [
        "# Loading & Preparing Data"
      ],
      "id": "a18VhnUanxhq"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "attached-chocolate",
        "outputId": "280498ac-528e-49f0-be07-256d9c7dd7c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HH8QFDcvkKfnj4dWmFQceb3PpNqDD8HQ&authuser=0&export=download\n",
            "To: /content/train.csv\n",
            "100% 171k/171k [00:00<00:00, 71.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uDOO8RP7Lr9qcRJO8z3d10qm_UggJv4I&authuser=0&export=download\n",
            "To: /content/eval.csv\n",
            "100% 43.5k/43.5k [00:00<00:00, 61.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1HH8QFDcvkKfnj4dWmFQceb3PpNqDD8HQ&authuser=0&export=download'\n",
        "!gdown 'https://drive.google.com/uc?id=1uDOO8RP7Lr9qcRJO8z3d10qm_UggJv4I&authuser=0&export=download'"
      ],
      "id": "attached-chocolate"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "christian-number"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "eval_df = pd.read_csv('eval.csv')"
      ],
      "id": "christian-number"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0Mcw7kGmoWyi",
        "outputId": "cb42e531-39c4-4bc6-caa0-a5d88f2a8009"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment</th>\n",
              "      <th>rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2587</td>\n",
              "      <td>پردازنده های Core i5 و Core i3 نیز ذاتا دو هست...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22591</td>\n",
              "      <td>سلام به دوستای عزیزم \\nعزاداری هاتون قبول باشه</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>141037</td>\n",
              "      <td>کلا پولتون رو دور نریزیزد</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58593</td>\n",
              "      <td>از صمیم قلب امیدوارم دایانا با کارن بمونه و پو...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5712</td>\n",
              "      <td>آنطور که اپل ادعا می کند آیپاد شافل دارای طراح...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                            comment  rate\n",
              "0        2587  پردازنده های Core i5 و Core i3 نیز ذاتا دو هست...   0.0\n",
              "1       22591     سلام به دوستای عزیزم \\nعزاداری هاتون قبول باشه   1.0\n",
              "2      141037                          کلا پولتون رو دور نریزیزد  -1.0\n",
              "3       58593  از صمیم قلب امیدوارم دایانا با کارن بمونه و پو...   1.0\n",
              "4        5712  آنطور که اپل ادعا می کند آیپاد شافل دارای طراح...   1.0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ],
      "id": "0Mcw7kGmoWyi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L4tBlfcToYmg",
        "outputId": "78b5e684-f6ce-41d1-cea2-d481f44b4463"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>comment</th>\n",
              "      <th>rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>61591</td>\n",
              "      <td>کیفیت غذا و زمان رسیدن عالی بود</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50299</td>\n",
              "      <td>در‌ حد ساندویچ یه نفره بود نه دونفره یا بمب. ک...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2777</td>\n",
              "      <td>طعم پیتزای چهار فصل مثل همشه خیلی خوب بود اما ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9126</td>\n",
              "      <td>مشخصات سخت افزاری مناسب در کنار سیستم عامل وین...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7544</td>\n",
              "      <td>مرغش سوخاری و خوشمزه بود، بسته بندی عالی، قیمت...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                            comment  rate\n",
              "0       61591                    کیفیت غذا و زمان رسیدن عالی بود  -1.0\n",
              "1       50299  در‌ حد ساندویچ یه نفره بود نه دونفره یا بمب. ک...   1.0\n",
              "2        2777  طعم پیتزای چهار فصل مثل همشه خیلی خوب بود اما ...  -1.0\n",
              "3        9126  مشخصات سخت افزاری مناسب در کنار سیستم عامل وین...   0.5\n",
              "4        7544  مرغش سوخاری و خوشمزه بود، بسته بندی عالی، قیمت...  -1.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_df.head()"
      ],
      "id": "L4tBlfcToYmg"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9hppW6kwoc9N"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(train_df.columns[0], axis=1)\n",
        "eval_df = eval_df.drop(eval_df.columns[0], axis=1)"
      ],
      "id": "9hppW6kwoc9N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIVevpfPonAK",
        "outputId": "233e883d-8f34-4c75-c631-1acc60687d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104 out of 800 train comments have rate zero.\n",
            "30 out of 200 eval comments have rate zero.\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_df[train_df.rate == 0])} out of {len(train_df)} train comments have rate zero.\")\n",
        "print(f\"{len(eval_df[eval_df.rate == 0])} out of {len(eval_df)} eval comments have rate zero.\")"
      ],
      "id": "RIVevpfPonAK"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fN3mSVdDp656"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[train_df.rate != 0]\n",
        "eval_df  = eval_df[eval_df.rate != 0]"
      ],
      "id": "fN3mSVdDp656"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7QKtjCFpo9xI"
      },
      "outputs": [],
      "source": [
        "train_df['rate'] = train_df.rate.apply(lambda r: 1 if r > 0 else 0)\n",
        "eval_df['rate'] = eval_df.rate.apply(lambda r: 1 if r > 0 else 0)"
      ],
      "id": "7QKtjCFpo9xI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ppN5iFXqUqC"
      },
      "source": [
        "## Balancing Dataset\n",
        "As you can see below, data is imbalanced. We use over-sampling strategy on negative class to mitigate the problem."
      ],
      "id": "6ppN5iFXqUqC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "7UcccdyZrvkJ",
        "outputId": "00abcb47-05f6-4b28-9a1b-3781ab13ecce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>502</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>194</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train  eval\n",
              "1    502   115\n",
              "0    194    55"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.concat([train_df.rate.value_counts().rename('train'),\n",
        "           eval_df.rate.value_counts().rename('eval')], axis=1)"
      ],
      "id": "7UcccdyZrvkJ"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kawgs4h-q01l"
      },
      "outputs": [],
      "source": [
        "balancer = RandomOverSampler(random_state=41)\n",
        "train_df, _ = balancer.fit_resample(train_df, train_df.rate)\n",
        "eval_df, _ = balancer.fit_resample(eval_df, eval_df.rate)"
      ],
      "id": "Kawgs4h-q01l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_EwWGNEtZpE"
      },
      "source": [
        "## Normalization"
      ],
      "id": "4_EwWGNEtZpE"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eCIhQtnNtcqC"
      },
      "outputs": [],
      "source": [
        "params = {'to_ascii':False, 'no_urls':True,    'no_phone_numbers':True, 'no_line_breaks':True,\n",
        "          'no_emails':True, 'no_numbers':True, 'no_digits':True,        'no_currency_symbols':True}\n",
        "\n",
        "train_df['comment'] = train_df.comment.apply(lambda c: cleantext.clean(c,**params))\n",
        "eval_df['comment'] = eval_df.comment.apply(lambda c: cleantext.clean(c,**params))"
      ],
      "id": "eCIhQtnNtcqC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFF6krZpV2w"
      },
      "source": [
        "## Vocabulary"
      ],
      "id": "GNFF6krZpV2w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PKFFeA7pV2x",
        "outputId": "8eebb18e-438f-4e8e-be7d-5ae1b1e67eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60669"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"vocab.txt\") as fp:\n",
        "    words = set([w.strip() for w in fp.readlines()][2:])\n",
        "len(words)"
      ],
      "id": "9PKFFeA7pV2x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QUS-j8-pV2z"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "for c in pd.concat([train_df['comment'], eval_df['comment']]):\n",
        "    words.update(word_tokenize(c))"
      ],
      "id": "6QUS-j8-pV2z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3gqdDYspV2z"
      },
      "outputs": [],
      "source": [
        "vocab = sorted(words)"
      ],
      "id": "o3gqdDYspV2z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCqHTGwEpV21"
      },
      "outputs": [],
      "source": [
        "with open('vocab.txt', 'w') as fw:\n",
        "    for w in vocab:\n",
        "        fw.write(w + '\\n')"
      ],
      "id": "qCqHTGwEpV21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ2eHD_DpV22",
        "outputId": "811716d4-a648-482e-8da8-be708dac8f53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60671"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ],
      "id": "NJ2eHD_DpV22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcU7TIPZ2pfG"
      },
      "source": [
        "# Method 1: Linear Models\n",
        "We make use of Logistic Regression and SVM as classifiers, and for vectorizing the comments, Tfidf is used."
      ],
      "id": "xcU7TIPZ2pfG"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "v2_juBOy3xIW"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=600, ngram_range=(1,3))"
      ],
      "id": "v2_juBOy3xIW"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XOPXtNPEww7e",
        "outputId": "e5ef5b70-3ddf-421a-9373-ed0b3d8aa918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('embedding',\n",
              "                 TfidfVectorizer(max_features=600, ngram_range=(1, 3))),\n",
              "                ('classifier', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "pipeline = Pipeline([('embedding', vectorizer),\n",
        "                     ('classifier', LinearSVC())])\n",
        "pipeline.fit(train_df.comment, train_df.rate)"
      ],
      "id": "XOPXtNPEww7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHTNpGAl4LDP"
      },
      "source": [
        "## Hyper-parameter Tuning\n",
        "We search over different settings and find the best."
      ],
      "id": "SHTNpGAl4LDP"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6_XBYuBT4R0V"
      },
      "outputs": [],
      "source": [
        "param_grid = {'embedding__ngram_range': [(1,2),(1,3),(1,4)],\n",
        "              'embedding__max_features': range(100, 3000, 100),\n",
        "              'classifier': [LinearSVC(),LogisticRegression()]}"
      ],
      "id": "6_XBYuBT4R0V"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rnv3weP4-gP",
        "outputId": "401d7e20-e563-4453-85da-c8ef79e650f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 174 candidates, totalling 174 fits\n"
          ]
        }
      ],
      "source": [
        "# The smelling code here is due to Scikit GridSearchCV's specific input for `cv` parameter.\n",
        "# GridSearchCV and other meta-estimators in Scikit accept whole data (train+eval) in their `fit`\n",
        "# method. So if you have a dataset separated in train and eval parts beforehand, you should\n",
        "# concatenate them. Beside that you have to give indices of train and eval parts as the `cv` parameter.\n",
        "\n",
        "train_eval = pd.concat((train_df, eval_df), ignore_index=True)\n",
        "train_eval_indices = [(train_df.index, eval_df.index + len(train_df))]\n",
        "meta_estimator = GridSearchCV(pipeline, param_grid, scoring=['accuracy', 'f1'],\n",
        "                              cv=train_eval_indices, refit='f1', verbose=1)\n",
        "_ = meta_estimator.fit(train_eval.comment, train_eval.rate)"
      ],
      "id": "3rnv3weP4-gP"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxJhaX4YDuhs",
        "outputId": "18a8ce1e-c9ba-4b52-842f-9483ef864047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classifier': LinearSVC(),\n",
              " 'embedding__max_features': 600,\n",
              " 'embedding__ngram_range': (1, 3)}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "meta_estimator.best_params_"
      ],
      "id": "IxJhaX4YDuhs"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsTd-cnTD8SJ",
        "outputId": "46fb5bcf-c04b-4ef9-e58a-80c99c966eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model F1: 0.7063197026022304\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best model F1: {meta_estimator.best_score_}\")"
      ],
      "id": "VsTd-cnTD8SJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obiJVnpEkbe"
      },
      "source": [
        "## Determining Marker Features"
      ],
      "id": "3obiJVnpEkbe"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rSqPMgiBEn9H",
        "outputId": "4a0dcc2a-1e62-403f-84fc-4676ea80548f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ارسال', 'اینکه', 'لک', 'معمولی', 'من خیلی', 'میدان', 'نيست',\n",
              "       'هم که', 'چندان', 'کاملا'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "selector = SelectFromModel(pipeline, threshold=-np.inf, max_features=10, prefit=True,\n",
        "                           importance_getter='named_steps.classifier.coef_')\n",
        "\n",
        "pipeline['embedding'].get_feature_names_out()[selector.get_support()]"
      ],
      "id": "rSqPMgiBEn9H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXg2hxaADT0m"
      },
      "source": [
        "# Method 2: Neural Networks"
      ],
      "id": "IXg2hxaADT0m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTb3Yk6_DY69"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ],
      "id": "MTb3Yk6_DY69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Ezvx12pV2_"
      },
      "outputs": [],
      "source": [
        "EMBED_DIM = 200\n",
        "SEQ_LEN = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "BATCH_SIZE = 20\n",
        "CLASS_NO = 2"
      ],
      "id": "f_Ezvx12pV2_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVM0izMNpV3A"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_df['comment'], train_df['rate'])).shuffle(buffer_size=len(train_df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "eval_ds = tf.data.Dataset.from_tensor_slices((eval_df['comment'], eval_df['rate'])).shuffle(buffer_size=len(eval_df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "id": "aVM0izMNpV3A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQgoCpv5pV3B"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization, Embedding, Input, Dense, Bidirectional, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_lstm_model(lstm_dim=64, dense_dim=16):\n",
        "    vectorizer = TextVectorization(vocabulary='vocab.txt',\n",
        "                                   output_mode='int', \n",
        "                                   output_sequence_length=SEQ_LEN)\n",
        "    return Sequential([\n",
        "        Input(shape=(1,), dtype=tf.string),\n",
        "        vectorizer,\n",
        "        Embedding(VOCAB_SIZE + 4, EMBED_DIM, name='embedding'),\n",
        "        Bidirectional(LSTM(lstm_dim)),\n",
        "        Dense(dense_dim, activation='relu'),\n",
        "        Dense(CLASS_NO, 'softmax')\n",
        "    ])"
      ],
      "id": "xQgoCpv5pV3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOdsKu78pV3E"
      },
      "outputs": [],
      "source": [
        "model = create_lstm_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "id": "NOdsKu78pV3E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajmZanWfpV3F",
        "outputId": "990940ac-e627-412e-8041-9c88c58978ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_16 (Text  (None, 64)               0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 64, 200)           12135000  \n",
            "                                                                 \n",
            " bidirectional_14 (Bidirecti  (None, 128)              135680    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 16)                2064      \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,272,778\n",
            "Trainable params: 12,272,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "ajmZanWfpV3F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Iv0EO4pV3G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max')"
      ],
      "id": "y1Iv0EO4pV3G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlFhlPYlpV3H",
        "outputId": "433d1219-ddf0-4f8c-deb1-1bed515deba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 15s 218ms/step - loss: 0.6750 - accuracy: 0.5817 - val_loss: 0.6640 - val_accuracy: 0.5739\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 10s 202ms/step - loss: 0.3038 - accuracy: 0.8835 - val_loss: 0.7537 - val_accuracy: 0.6522\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f41994e7700>"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=eval_ds, epochs=10, callbacks=[es])"
      ],
      "id": "hlFhlPYlpV3H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6iqEzrQDYG5"
      },
      "source": [
        "# Method 3: Pre-trained Language Models"
      ],
      "id": "w6iqEzrQDYG5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NG5EEcYELKN"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "8NG5EEcYELKN"
    }
  ],
  "metadata": {
    "colab": {
      "name": "sentiment-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}